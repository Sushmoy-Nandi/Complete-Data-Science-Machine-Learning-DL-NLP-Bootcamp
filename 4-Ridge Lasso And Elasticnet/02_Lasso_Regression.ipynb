{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae619218",
   "metadata": {},
   "source": [
    "# Lasso Regression (L1 Regularization)\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "* **Lasso Regression**, also known as **L1 Regularization**, is used for **feature selection**.\n",
    "* It modifies the cost function by adding a penalty term: **Lambda ($\\lambda$)** multiplied by the **magnitude of the slope**.\n",
    "* A key property of Lasso Regression is that it can reduce the coefficients of less important features to exactly **zero**.\n",
    "* This zeroing out of coefficients effectively removes the corresponding feature from the model, making it ideal for datasets with many features.\n",
    "\n",
    "## Exam Notes\n",
    "\n",
    "### Feature Selection vs. Overfitting\n",
    "\n",
    "**Question:** Why do we use Lasso Regression compared to Ridge Regression?\n",
    "\n",
    "**Answer:**  \n",
    "While Ridge Regression is primarily used to reduce overfitting, **Lasso Regression** is specifically used for **feature selection**.  \n",
    "It helps identify and retain only the most important features in a model.\n",
    "\n",
    "### Relationship Between Lambda and Slope\n",
    "\n",
    "**Question:** What happens to the slope when Lambda increases in Lasso Regression?\n",
    "\n",
    "**Answer:**  \n",
    "As **Lambda ($\\lambda$)** increases, the **slope ($\\theta$)** decreases.  \n",
    "Unlike Ridge Regression, in Lasso Regression the slope can eventually become **zero**, indicating that the corresponding feature has been removed from the model.\n",
    "\n",
    "## Lasso Regression Details\n",
    "\n",
    "**Lasso Regression** (Least Absolute Shrinkage and Selection Operator) is a linear regression technique that includes a regularization term.  \n",
    "It is also referred to as **L1 Regularization**.\n",
    "\n",
    "### The Cost Function\n",
    "\n",
    "The cost function for Lasso Regression is the standard Mean Squared Error (MSE) plus a penalty term based on the **absolute value** of the slopes.\n",
    "\n",
    "$$\n",
    "J(\\theta) =\n",
    "\\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "+\n",
    "\\lambda \\sum_{i=1}^{n} |\\ Slope|\n",
    "$$\n",
    "\n",
    "* **First Term:** Standard Mean Squared Error (MSE) from linear regression.\n",
    "* **Second Term:** Regularization penalty using the **absolute value** of coefficients (L1 norm), not the square as in Ridge Regression.\n",
    "\n",
    "### Feature Selection Mechanism\n",
    "\n",
    "The primary use case for Lasso Regression is **feature selection**.\n",
    "\n",
    "* **Unimportant Features:**  \n",
    "  Features with weak correlation to the output tend to have their coefficients shrunk to **zero**.\n",
    "\n",
    "* **Important Features:**  \n",
    "  Features strongly correlated with the output retain **non-zero** coefficients.\n",
    "\n",
    "### Example Scenario\n",
    "\n",
    "Consider a model with four features: $x_1, x_2, x_3, x_4$.\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\theta_4 x_4\n",
    "$$\n",
    "\n",
    "Suppose the initial coefficients are:\n",
    "\n",
    "* $\\theta_1 = 0.65$\n",
    "* $\\theta_2 = 0.72$\n",
    "* $\\theta_3 = 0.034$\n",
    "* $\\theta_4 = 0.12$\n",
    "\n",
    "Here, $x_4$ has a very small coefficient, indicating weak correlation with the output.  \n",
    "When Lasso Regression is applied, this coefficient is penalized and reduced to **zero**.\n",
    "\n",
    "The equation becomes:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + 0 \\times x_4\n",
    "$$\n",
    "\n",
    "The term $0 \\times x_4$ disappears, effectively removing feature $x_4$ from the model.\n",
    "\n",
    "### Relationship Between Lambda ($\\lambda$) and Slope ($\\theta$)\n",
    "\n",
    "Understanding the relationship between **Lambda** and the **Slope** explains how feature selection occurs.\n",
    "\n",
    "1. **$\\lambda = 0$**  \n",
    "   * No penalty is applied  \n",
    "   * Model behaves like standard Linear Regression\n",
    "\n",
    "2. **$\\lambda$ Increases**  \n",
    "   * Penalty grows stronger  \n",
    "   * Coefficients begin to shrink\n",
    "\n",
    "3. **Coefficient Becomes Zero**  \n",
    "   * At sufficiently large $\\lambda$, some coefficients become **exactly zero**\n",
    "   * Corresponding features are removed from the model\n",
    "\n",
    "This behavior makes **Lasso Regression** an effective automatic feature selection technique, especially useful for datasets with a large number of input features.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
