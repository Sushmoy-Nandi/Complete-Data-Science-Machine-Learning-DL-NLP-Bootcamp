{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4159c9",
   "metadata": {},
   "source": [
    "# Types of Cross Validation\n",
    "\n",
    "## Commands\n",
    "\n",
    "* `GridSearchCV`\n",
    "  * Used for hyperparameter tuning to find the optimal parameters for a model.\n",
    "* `RandomizedSearchCV`\n",
    "  * An alternative method for hyperparameter tuning that selects parameters randomly.\n",
    "* `train_test_split(random_state=<value>)`\n",
    "  * Used to split datasets; changing the `random_state` value results in different data splits and potentially different accuracy scores.\n",
    "\n",
    "## Summary\n",
    "\n",
    "* **Data Splitting Necessity**: Data is initially divided into **training** and **test** sets. The training set is further split into **train** and **validation** sets to perform model training and hyperparameter tuning.\n",
    "* **Purpose of Cross Validation**: It addresses the issue of varying model accuracy caused by different random states during data splitting. It allows data scientists to determine the **average accuracy** of a model.\n",
    "* **Leave One Out CV (LOOCV)**: Uses a single record for validation and the rest for training. It is computationally expensive and prone to **overfitting** due to high training data variance.\n",
    "* **Leave P Out CV**: Similar to LOOCV but reserves **P** records for validation (e.g., 10, 20, 30) instead of just one.\n",
    "* **K-Fold CV**: Splits the dataset into **K** equal parts (folds). The model iterates K times, using one fold for validation and the rest for training each time, providing an average accuracy.\n",
    "* **Stratified K-Fold CV**: An improvement on K-Fold that ensures the **validation data** maintains the same proportion of target class labels (e.g., binary outputs) as the original dataset, preventing imbalanced validation sets.\n",
    "* **Time Series CV**: Designed for time-dependent data (e.g., product reviews). It splits data chronologically (e.g., Day 1-4 for train, Day 5 for validation) rather than randomly, preserving the temporal sequence.\n",
    "\n",
    "## Introduction to Model Evaluation\n",
    "\n",
    "### Data Splitting Strategy\n",
    "When training a machine learning model, the dataset is typically divided into two primary parts:\n",
    "* **Training Data**: Used to train the model.\n",
    "* **Test Data**: Used exclusively to check the performance of the model on **new data**. This data is never shown to the model during the training phase.\n",
    "\n",
    "### Hyperparameter Tuning and Validation\n",
    "To optimize the model, the **training data** is further split into **train** and **validation** sets.\n",
    "* **Validation Data**: This subset is used for **hyperparameter tuning** (playing with multiple parameters) and validating the model during the training process.\n",
    "* **Performance Metrics**: After training and validation are complete, the model's performance is evaluated on the test data using metrics such as **accuracy**, **precision**, **recall**, and **mean squared error**.\n",
    "\n",
    "### The Need for Cross Validation\n",
    "When splitting training data into train and validation sets, an important parameter called **random state** controls the split. Changing this value alters which records end up in which set, leading to fluctuating accuracy scores (e.g., 85% in one split, 92% in another). **Cross Validation** allows us to calculate the **average accuracy** across multiple splits, providing a more reliable measure of model performance.\n",
    "\n",
    "## Types of Cross Validation techniques\n",
    "\n",
    "### 1. Leave One Out Cross Validation (LOOCV)\n",
    "In this technique, the model is trained and validated multiple times based on the total number of records ($N$).\n",
    "* **Process**: For every experiment, **one record** is used as the validation set, and the remaining records are used as the training set.\n",
    "* **Experiments**: If the dataset has 500 records, 500 separate experiments are performed.\n",
    "* **Disadvantages**:\n",
    "    * **High Complexity**: As data size increases, the computational cost of training the model effectively multiplies by the number of records (e.g., 5000 records require 5000 experiments).\n",
    "    * **Overfitting**: Since the model trains on nearly the entire dataset (N-1 records) for every iteration, training accuracy is high, but validation accuracy may be low, leading to poor performance on new test data.\n",
    "\n",
    "### 2. Leave P Out Cross Validation\n",
    "This is a variation of LOOCV where **P** records are left out for validation instead of just one.\n",
    "* **Process**: **P** can be set to values like 10, 20, or 30.\n",
    "* The remaining process remains the same as LOOCV, but the validation set size is slightly larger.\n",
    "\n",
    "### 3. K-Fold Cross Validation\n",
    "This is a widely used technique that splits the data into **K** distinct sections or \"folds.\"\n",
    "* **Process**: The total number of records ($n$) is divided by $K$ to determine the test size for each fold.\n",
    "    * *Example*: If $n=500$ and $K=5$, the test size is $100$ records.\n",
    "* **Iteration**:\n",
    "    * **Experiment 1**: The first 100 records are the **validation data**; the remaining 400 are training data.\n",
    "    * **Experiment 2**: The next 100 records become validation data, and the rest are training.\n",
    "    * This continues until all folds have been used as validation data exactly once.\n",
    "* **Outcome**: An accuracy score is calculated for each experiment, and the **average accuracy** is derived from all $K$ experiments.\n",
    "\n",
    "### 4. Stratified K-Fold Cross Validation\n",
    "This method addresses a specific flaw in standard K-Fold Cross Validation regarding **imbalanced datasets**, particularly in classification problems.\n",
    "* **The Problem**: In standard K-Fold, a random split might result in a validation set containing only one type of class category (e.g., all 1s or all 0s in a binary problem). This prevents the model from learning properly.\n",
    "* **The Solution**: Stratified K-Fold ensures that the **validation data** maintains the same **proportion** of target classes (e.g., a 60:40 ratio of 1s to 0s) as found in the original dataset.\n",
    "* **Benefit**: It guarantees that every validation fold is a representative sample of the overall data distribution.\n",
    "\n",
    "### 5. Time Series Cross Validation\n",
    "This technique is essential for **time-dependent data**, such as product sentiment analysis or stock prices, where the order of data points matters.\n",
    "* **Context**: Reviews or data points may change nature over time (e.g., product features improve from January to December).\n",
    "* **Constraint**: You cannot randomly split time-series data. The validation set must always come *after* the training set chronologically.\n",
    "* **Process**:\n",
    "    * Data is split based on time steps (e.g., days).\n",
    "    * **Split Example**: Day 1 to Day 4 serves as **training data**, while Day 4 to Day N serves as **validation data**.\n",
    "    * The sequence is strictly maintained; future data is never used to train for past predictions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
