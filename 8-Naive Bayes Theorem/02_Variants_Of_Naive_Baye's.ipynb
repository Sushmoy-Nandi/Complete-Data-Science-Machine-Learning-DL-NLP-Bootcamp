{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "060bfdea",
   "metadata": {},
   "source": [
    "# Variants of Naive Bayes Algorithm\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "* The Naive Bayes algorithm has three main variants: **Bernoulli**, **Multinomial**, and **Gaussian**.\n",
    "* The choice of which variant to use depends entirely on the type of data and the distribution of the independent features in the dataset.\n",
    "* **Bernoulli Naive Bayes** is ideal when the independent features follow a Bernoulli distribution, meaning the features are binary or have only two possible outcomes (e.g., yes/no, pass/fail, true/false).\n",
    "* **Multinomial Naive Bayes** is specifically designed for text classification problems, such as spam detection, where text data must first be converted into numerical vectors using Natural Language Processing (NLP) techniques.\n",
    "* \n",
    "**Gaussian Naive Bayes** is used when the independent features contain continuous numerical values that follow (or can be transformed to follow) a Gaussian or normal distribution (a bell curve). \n",
    "\n",
    "\n",
    "\n",
    "## Types of Naive Bayes Classifiers\n",
    "\n",
    "### 1. Bernoulli Naive Bayes\n",
    "\n",
    "**Bernoulli Naive Bayes** is used to solve classification problems when the independent features in a dataset follow a **Bernoulli distribution**. \n",
    "\n",
    "A Bernoulli distribution describes a scenario where there are only two possible outcomes. A classic real-world example is tossing a coin, which can only result in heads or tails.  In the context of a dataset, this means the features are binary. Examples of Bernoulli features include:\n",
    "\n",
    "* Pass or Fail \n",
    "\n",
    "\n",
    "* Yes or No \n",
    "\n",
    "\n",
    "* Male or Female \n",
    "\n",
    "\n",
    "* 0 or 1 \n",
    "\n",
    "\n",
    "\n",
    "If a dataset is overwhelmingly populated with features that have only two categories, Bernoulli Naive Bayes is the optimal choice for binary or multi-class classification. \n",
    "\n",
    "### 2. Multinomial Naive Bayes\n",
    "\n",
    "**Multinomial Naive Bayes** is primarily used when the input data is in the form of **text**. A standard example of this is a **spam classification** model, where the algorithm evaluates the body of an email to predict if it is \"Spam\" or \"Ham\" (not spam). \n",
    "\n",
    "Because machine learning models cannot natively understand text sentences, the raw text data must first be converted into numerical values, or **vectors**. This conversion process utilizes **Natural Language Processing (NLP)** techniques.  Common NLP techniques for vectorizing text include:\n",
    "\n",
    "* **Bag of Words (BoW)** \n",
    "\n",
    "\n",
    "* **TF-IDF** (Term Frequency-Inverse Document Frequency) \n",
    "\n",
    "\n",
    "* **Word2Vec** \n",
    "\n",
    "\n",
    "\n",
    "These techniques rely on formulas that evaluate text structure, such as calculating the total number of words or the frequency of unique words in a document. Multinomial Naive Bayes is designed to handle the numerical data generated by these specific NLP vectorization formulas. \n",
    "\n",
    "### 3. Gaussian Naive Bayes\n",
    "\n",
    "**Gaussian Naive Bayes** is implemented when the independent features contain **continuous values** that follow a **Gaussian distribution**. \n",
    "\n",
    "A Gaussian distribution, commonly known as a **normal distribution** or **bell curve**, represents continuous numerical data. A classic example is the Iris dataset, where features like sepal length and petal width are continuous measurements.  Other common examples of continuous features include:\n",
    "\n",
    "* Age \n",
    "\n",
    "\n",
    "* Height \n",
    "\n",
    "\n",
    "* Weight \n",
    "\n",
    "\n",
    "\n",
    "While ideally the features should naturally form a bell curve, Gaussian Naive Bayes can still be effective if the data is slightly left-skewed or right-skewed. Furthermore, if features follow a different distribution (like an exponential distribution), they can often be transformed into a normal distribution using mathematical formulas before applying the algorithm. \n",
    "\n",
    "If a dataset contains a mix of continuous features and multi-category features (non-binary), Gaussian Naive Bayes is generally the best approach."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
