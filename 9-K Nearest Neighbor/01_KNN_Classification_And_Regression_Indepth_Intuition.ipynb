{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "424d72e9",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbor (KNN) Algorithm\n",
    "\n",
    "## Summary\n",
    "\n",
    "* **K-Nearest Neighbor (KNN)** is a straightforward machine learning algorithm used to solve both **classification** and **regression** problem statements.\n",
    "\n",
    "\n",
    "* The algorithm functions by calculating the distance between a new test data point and existing training data points to find its closest neighbors.\n",
    "\n",
    "\n",
    "* For **classification** tasks, the model predicts the output by determining the majority category among the $K$ nearest neighbors.\n",
    "\n",
    "\n",
    "* For **regression** tasks, the model predicts a continuous output by calculating the **average** or **median** of the target values of the $K$ nearest neighbors.\n",
    "\n",
    "\n",
    "* Distance calculations typically rely on metrics such as **Euclidean Distance** or **Manhattan Distance**.\n",
    "\n",
    "\n",
    "* To optimize the $O(n)$ **time complexity** encountered with large datasets, algorithm variants like the **KD Tree** and **Ball Tree** are utilized.\n",
    "\n",
    "\n",
    "\n",
    "## KNN Classification Process\n",
    "\n",
    "The **classification** process in KNN determines the category of a new data point based on its proximity to the existing training data. The target variable can be a binary category (e.g., 0 or 1) or a multi-class category. The methodology follows three primary steps:\n",
    "\n",
    "* **Step 1: Initialize the $K$ value**: The $K$ value represents the number of neighbors to consider and must be a number greater than zero. This value acts as a **hyperparameter**, meaning it changes based on the specific dataset. The optimal $K$ value is selected through **hyperparameter tuning** to identify which number of neighbors yields the highest model accuracy.\n",
    "\n",
    "\n",
    "* **Step 2: Find the $K$ Nearest Neighbors**: For a new test data point, the algorithm calculates distances to the training dataset and identifies the top $K$ closest points.\n",
    "\n",
    "\n",
    "* **Step 3: Determine the Majority Category**: The algorithm counts how many of the $K$ nearest neighbors belong to each distinct category. The final predicted output for the test data point is the category containing the maximum number of neighbors.\n",
    "\n",
    "\n",
    "\n",
    "## Distance Metrics\n",
    "\n",
    "To identify the nearest neighbors, the algorithm must calculate the distance between the test data point and the training data points using specific distance formulas. The choice between these metrics depends heavily on the specific problem statement, and **hyperparameter tuning** is often used to check which metric provides higher accuracy.\n",
    "\n",
    "### Euclidean Distance\n",
    "\n",
    "**Euclidean Distance** calculates the direct, straight-line distance (hypotenuse) between coordinates.\n",
    "\n",
    "* In a two-dimensional space with data points $(x_1, y_1)$ and $(x_2, y_2)$ , the mathematical formula is $\\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}$.\n",
    "\n",
    "\n",
    "* In a three-dimensional space, an additional $(z_2 - z_1)^2$ term is added to the root.\n",
    "\n",
    "\n",
    "* This metric is conceptually similar to how flight distances are calculated between cities in the air.\n",
    "\n",
    "\n",
    "\n",
    "### Manhattan Distance\n",
    "\n",
    "**Manhattan Distance** calculates distance by traveling along the axes (the base and the adjacent sides) rather than computing a direct straight line.\n",
    "\n",
    "* This metric is comparable to navigating a grid-like city layout (such as city blocks in the US), where a vehicle like an Uber must travel along structured roads and blocks rather than flying point-to-point.\n",
    "\n",
    "\n",
    "\n",
    "## KNN Regression Process\n",
    "\n",
    "The **KNN regression** process shares the exact same foundational steps as classification but is applied to predict continuous values, such as predicting a house price based on its size.\n",
    "\n",
    "* The process still requires initializing a $K$ value and locating the $K$ nearest neighbors for the new test data point.\n",
    "\n",
    "\n",
    "* Instead of categorizing based on a majority vote, the algorithm combines the target values of those nearest neighbors and calculates their **average** to determine the final continuous output.\n",
    "\n",
    "\n",
    "* If the dataset contains a significant number of **outliers**, using the **median** value of the neighbors is a robust alternative to the average.\n",
    "\n",
    "\n",
    "\n",
    "## Time Complexity and KNN Variants\n",
    "\n",
    "A critical challenge with the standard KNN algorithm is its **time complexity**.\n",
    "\n",
    "* Calculating the distance between a new test data point and every individual point in the training dataset requires an $O(n)$ time complexity.\n",
    "\n",
    "\n",
    "* This exhaustive calculation process becomes highly inefficient and cumbersome when processing millions of data points.\n",
    "\n",
    "\n",
    "\n",
    "### KD Tree and Ball Tree\n",
    "\n",
    "To optimize distance calculations and mitigate high time complexity, variants of the algorithm such as the **KD Tree** and **Ball Tree** are utilized.\n",
    "\n",
    "* These techniques structure the data points using a **Binary Tree** format.\n",
    "\n",
    "\n",
    "* By grouping data hierarchically, these tree structures drastically reduce the need to calculate the distance to every single data point, optimizing the search for the top 5 or 10 nearest neighbors."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
