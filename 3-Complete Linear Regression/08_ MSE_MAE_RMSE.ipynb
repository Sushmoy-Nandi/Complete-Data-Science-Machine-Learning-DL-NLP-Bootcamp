{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81414910",
   "metadata": {},
   "source": [
    "# Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE)\n",
    "\n",
    "## Commands\n",
    "\n",
    "* `MSE = (1/n) * Σ(y - y_hat)^2`\n",
    "* `MAE = (1/n) * Σ|y - y_hat|`\n",
    "* `RMSE = sqrt(MSE)`\n",
    "* `RMSE = sqrt((1/n) * Σ(y - y_hat)^2)`\n",
    "\n",
    "## Summary\n",
    "\n",
    "* **Mean Squared Error (MSE)** measures the average squared difference between the estimated values and the actual value, emphasizing larger errors by squaring them.\n",
    "* **Mean Absolute Error (MAE)** calculates the average of the absolute differences between prediction and actual observation, providing a linear score that weights all differences equally.\n",
    "* **Root Mean Squared Error (RMSE)** is the square root of MSE, bringing the error metric back to the same unit as the target variable for easier interpretation.\n",
    "* **MSE** is differentiable and converges faster due to its convex nature but is sensitive to outliers.\n",
    "* **MAE** is robust to outliers but is computationally more complex to optimize because it is not differentiable at zero.\n",
    "\n",
    "## Exam Notes\n",
    "\n",
    "### Comparing MSE and MAE\n",
    "\n",
    "**Question**: When should you use **MSE** compared to **MAE**?\n",
    "\n",
    "**Answer**:  \n",
    "Use **MSE** when you need a loss function that is **differentiable** at all points and want faster convergence. MSE creates a **quadratic curve** (convex function) with a single global minima, making optimization efficient. However, avoid MSE if your dataset has many **outliers**, as squaring the error penalizes them heavily and skews the model.\n",
    "\n",
    "**Question**: When is **MAE** preferred over **MSE**?\n",
    "\n",
    "**Answer**:  \n",
    "**MAE** is preferred when your dataset contains **outliers** that you do not want to heavily influence the model. It is **robust to outliers** because it takes the absolute difference rather than squaring it. The trade-off is that MAE is **not differentiable at zero** (requires sub-gradients) and typically takes longer to converge.\n",
    "\n",
    "---\n",
    "\n",
    "## Mean Squared Error (MSE)\n",
    "\n",
    "**Mean Squared Error (MSE)** is a common loss function defined by the formula:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "### Advantages of MSE\n",
    "\n",
    "1. **Differentiable**:  \n",
    "   The squared term creates a quadratic equation (parabola), which is a **convex function**. This ensures differentiability at all points, allowing gradient descent to compute slopes effectively.\n",
    "\n",
    "2. **Single Global Minima**:  \n",
    "   Being convex, MSE has only one **global minima** and no local minima, preventing the optimizer from getting stuck.\n",
    "\n",
    "3. **Faster Convergence**:  \n",
    "   Gradient descent converges faster due to the smooth convex curve.\n",
    "\n",
    "### Disadvantages of MSE\n",
    "\n",
    "1. **Not Robust to Outliers**:  \n",
    "   Squaring magnifies large errors. Outliers can significantly distort the best fit line.\n",
    "\n",
    "2. **Unit Mismatch**:  \n",
    "   The unit of MSE is the square of the target variable’s unit, making interpretation less intuitive.\n",
    "\n",
    "## Mean Absolute Error (MAE)\n",
    "\n",
    "**Mean Absolute Error (MAE)** calculates the average absolute difference between predicted and actual values:\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "### Advantages of MAE\n",
    "\n",
    "1. **Robust to Outliers**:  \n",
    "   Absolute differences prevent large errors from dominating the loss.\n",
    "\n",
    "2. **Same Unit**:  \n",
    "   Error is expressed in the same unit as the target variable, making it easy to interpret.\n",
    "\n",
    "### Disadvantages of MAE\n",
    "\n",
    "1. **Slower Convergence**:  \n",
    "   Optimization is generally slower than MSE.\n",
    "\n",
    "2. **Not Differentiable at Zero**:  \n",
    "   The absolute value function has a sharp point at zero, requiring **sub-gradient methods**.\n",
    "\n",
    "## Root Mean Squared Error (RMSE)\n",
    "\n",
    "**Root Mean Squared Error (RMSE)** is the square root of MSE:\n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "* **Same Unit**:  \n",
    "  RMSE restores the original unit of the target variable, improving interpretability.\n",
    "\n",
    "* **Differentiable**:  \n",
    "  Retains optimization advantages similar to MSE.\n",
    "\n",
    "* **Not Robust to Outliers**:  \n",
    "  Since it is derived from MSE, it remains sensitive to outliers.\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Metric | Outlier Robustness | Differentiable? | Unit Match? | Convergence Speed |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **MSE** | No (Sensitive) | Yes | No (Squared) | Fast |\n",
    "| **MAE** | Yes (Robust) | No (at 0) | Yes | Slow |\n",
    "| **RMSE** | No (Sensitive) | Yes | Yes | Fast |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
