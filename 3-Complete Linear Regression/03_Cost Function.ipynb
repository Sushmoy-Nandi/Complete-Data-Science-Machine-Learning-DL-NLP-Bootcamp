{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc086424",
   "metadata": {},
   "source": [
    "# Simple Linear Regression - Cost Function and Gradient Descent\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "* The goal of simple linear regression is to find the **best fit line** by minimizing the error between predicted and actual values\n",
    "* The **cost function** used is **Mean Squared Error (MSE)**, denoted as J(θ₀, θ₁) = (1/2m) Σ(h_θ(x^(i)) - y^(i))²\n",
    "* **Theta zero (θ₀)** represents the **intercept** of the line, while **theta one (θ₁)** represents the **slope**\n",
    "* The cost function calculates the squared difference between predicted points and true output values, then averages them\n",
    "* Alternative cost functions exist, including **Mean Absolute Error** and **Root Mean Squared Error**\n",
    "* **Gradient Descent** is the curve formed when plotting the cost function against different parameter values\n",
    "* The **global minima** is the point on the gradient descent curve where the cost function is minimized\n",
    "* The aim is to reach the global minima by systematically changing θ₀ and θ₁ values\n",
    "* A **convergence algorithm** is needed to efficiently find the optimal parameter values rather than randomly selecting them\n",
    "\n",
    "## Cost Function in Simple Linear Regression\n",
    "\n",
    "The primary objective in simple linear regression is to find the **best fit line** that minimizes the total error across all data points. This is achieved through the use of a **cost function**.\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "The cost function used in this context is the **Mean Squared Error**, represented by the notation:\n",
    "\n",
    "**J(θ₀, θ₁) = (1/2m) Σ(h_θ(x^(i)) - y^(i))²**\n",
    "\n",
    "Where:\n",
    "* **m** is the number of training examples\n",
    "* **h_θ(x^(i))** represents the predicted points\n",
    "* **y^(i)** represents the true output values\n",
    "* The summation runs from i = 1 to m\n",
    "\n",
    "### Understanding the Components\n",
    "\n",
    "The term **h_θ(x^(i)) - y^(i)** calculates the **error value** by subtracting the true output from the predicted value. Squaring this difference serves multiple purposes and provides advantages that distinguish MSE from other cost functions.\n",
    "\n",
    "### Alternative Cost Functions\n",
    "\n",
    "While MSE is commonly used, other cost functions are also available:\n",
    "* **Mean Absolute Error (MAE)**\n",
    "* **Root Mean Squared Error (RMSE)**\n",
    "\n",
    "Each of these cost functions has specific advantages and disadvantages that are relevant in different scenarios.\n",
    "\n",
    "## The Optimization Objective\n",
    "\n",
    "The fundamental aim is to **minimize the cost function J(θ₀, θ₁)** by adjusting the values of theta zero and theta one. This minimization process involves:\n",
    "\n",
    "* **Theta zero (θ₀)**: Controls the intercept of the line\n",
    "* **Theta one (θ₁)**: Controls the slope of the line\n",
    "\n",
    "The optimization goal is expressed as:\n",
    "\n",
    "**Minimize J(θ₀, θ₁) = (1/2m) Σ(h_θ(x^(i)) - y^(i))²**\n",
    "\n",
    "Since the cost function divides by m (the number of examples), this produces a **mean** of the squared errors, hence the name Mean Squared Error.\n",
    "\n",
    "## The Line Equation\n",
    "\n",
    "The equation of the straight line in simple linear regression is:\n",
    "\n",
    "**h_θ(x) = θ₀ + θ₁ · x**\n",
    "\n",
    "This equation represents the best fit line that the algorithm attempts to find.\n",
    "\n",
    "## Simplified Example with θ₀ = 0\n",
    "\n",
    "To visualize the concept in a 2D diagram, consider the special case where **theta zero is set to zero**. This means:\n",
    "* The line passes through the **origin**\n",
    "* The **intercept is exactly zero**\n",
    "* The equation simplifies to: **h_θ(x) = θ₁ · x**\n",
    "\n",
    "### Example Dataset\n",
    "\n",
    "Consider a simple dataset:\n",
    "\n",
    "| x | y |\n",
    "|---|---|\n",
    "| 1 | 1 |\n",
    "| 2 | 2 |\n",
    "| 3 | 3 |\n",
    "\n",
    "For this dataset, the data points are (1,1), (2,2), and (3,3).\n",
    "\n",
    "### Testing Different θ₁ Values\n",
    "\n",
    "#### Case 1: θ₁ = 1 (Optimal)\n",
    "\n",
    "When **theta one equals 1**, the equation becomes:\n",
    "* **h_θ(x) = 1 · x**\n",
    "* For x = 1: h_θ(1) = 1\n",
    "* For x = 2: h_θ(2) = 2\n",
    "* For x = 3: h_θ(3) = 3\n",
    "\n",
    "The predicted points (1,1), (2,2), (3,3) match the actual data points perfectly. The cost function calculation:\n",
    "\n",
    "```\n",
    "J(θ₁) = (1/2m) Σ(h_θ(x^(i)) - y^(i))²\n",
    "J(θ₁) = (1/2·3)[(1-1)² + (2-2)² + (3-3)²]\n",
    "J(θ₁) = (1/6)[0 + 0 + 0]\n",
    "J(θ₁) = 0\n",
    "```\n",
    "\n",
    "This represents the **best fit line** for this dataset, with **zero error**. The line passes perfectly through all data points, creating the **global minima** on the gradient descent curve.\n",
    "\n",
    "#### Case 2: θ₁ = 0.5\n",
    "\n",
    "When **theta one equals 0.5**, the equation becomes:\n",
    "* **h_θ(x) = 0.5 · x**\n",
    "* For x = 1: h_θ(1) = 0.5\n",
    "* For x = 2: h_θ(2) = 1\n",
    "* For x = 3: h_θ(3) = 1.5\n",
    "\n",
    "The cost function calculation:\n",
    "\n",
    "```\n",
    "J(θ₁) = (1/2·3)[(0.5-1)² + (1-2)² + (1.5-3)²]\n",
    "J(θ₁) = (1/6)[(0.5)² + (1)² + (1.5)²]\n",
    "J(θ₁) = (1/6)[0.25 + 1 + 2.25]\n",
    "J(θ₁) ≈ 0.58\n",
    "```\n",
    "\n",
    "This yields approximately **J(θ₁) ≈ 0.58**, indicating a higher error than the optimal case.\n",
    "\n",
    "#### Case 3: θ₁ = 0\n",
    "\n",
    "When **theta one equals zero**, the equation becomes:\n",
    "* **h_θ(x) = 0**\n",
    "* All predicted values are zero\n",
    "* The predicted points are (1,0), (2,0), (3,0) instead of the actual (1,1), (2,2), (3,3)\n",
    "\n",
    "The cost function calculation:\n",
    "\n",
    "```\n",
    "J(θ₁) = (1/2·3)[(0-1)² + (0-2)² + (0-3)²]\n",
    "J(θ₁) = (1/6)[1 + 4 + 9]\n",
    "J(θ₁) = (1/6)[14]\n",
    "J(θ₁) ≈ 2.3\n",
    "```\n",
    "\n",
    "This produces a significantly higher error of approximately **2.3**, confirming this is not the best fit line.\n",
    "\n",
    "### Visualization of the Gradient Descent Curve\n",
    "\n",
    "When plotting the cost function **J(θ₁)** against different values of **θ₁**, we observe:\n",
    "\n",
    "* At **θ₁ = 0**: J(θ₁) ≈ 2.3 (high error)\n",
    "* At **θ₁ = 0.5**: J(θ₁) ≈ 0.58 (moderate error)\n",
    "* At **θ₁ = 1**: J(θ₁) = 0 (zero error - **global minima**)\n",
    "* At **θ₁ = 1.5**: Error increases again\n",
    "* At **θ₁ = 2.0**: Error continues to increase\n",
    "* At **θ₁ = 2.5**: Error is even higher\n",
    "\n",
    "This creates a **U-shaped curve** (parabola) where:\n",
    "* The lowest point (bottom of the U) is at θ₁ = 1\n",
    "* This point represents the **global minima**\n",
    "* Moving away from this point in either direction increases the error\n",
    "* The curve demonstrates the **gradient descent** concept\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "When plotting different **theta one values** against their corresponding **cost function values J(θ₁)**, a curve emerges. This curve is called **Gradient Descent**.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "* The curve shows how the cost function changes with different parameter values\n",
    "* The lowest point on this curve represents the **global minima**\n",
    "* At the global minima, the **error is minimized**\n",
    "* This point corresponds to the **best fit line**\n",
    "\n",
    "### The Global Minima\n",
    "\n",
    "The **global minima** is the point where:\n",
    "* The cost function reaches its minimum value\n",
    "* The error is at its lowest\n",
    "* The best fit line is achieved\n",
    "\n",
    "In the example above, the global minima occurs at **θ₁ = 1**, where **J(θ₁) = 0**.\n",
    "\n",
    "### Importance in Machine Learning\n",
    "\n",
    "**Gradient Descent** is not only crucial for simple linear regression but is also **super important in deep learning techniques**. The fundamental concept remains the same: systematically move toward the point that minimizes the cost function.\n",
    "\n",
    "## The Need for a Convergence Algorithm\n",
    "\n",
    "While it's possible to manually test different values of theta one (and theta zero), this approach is not practical because:\n",
    "* Randomly selecting different parameter values is inefficient\n",
    "* There's no systematic way to improve the estimates\n",
    "* Manual testing doesn't scale to larger datasets or more complex models\n",
    "\n",
    "A **convergence algorithm** is required to:\n",
    "* Start with an initial value of θ₁ (and θ₀)\n",
    "* Systematically change these values\n",
    "* Move toward the global minima efficiently\n",
    "* Find the optimal parameters that minimize the cost function\n",
    "\n",
    "The convergence algorithm provides a **mechanism for changing parameter values** in a way that consistently reduces the cost function and approaches the best fit line.\n",
    "\n",
    "## Moving Forward\n",
    "\n",
    "The key takeaway is understanding that:\n",
    "* The goal is to minimize the cost function **J(θ₀, θ₁)**\n",
    "* This is achieved by moving along the **gradient descent curve**\n",
    "* The target is to reach the **global minima**\n",
    "* A systematic **convergence algorithm** is needed to accomplish this efficiently\n",
    "\n",
    "The convergence algorithm methodology will determine how to update theta zero and theta one values to reach the optimal solution.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
