{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a9a274b",
   "metadata": {},
   "source": [
    "# Performance Metrics: Confusion Matrix, Accuracy, Precision, Recall, and F-Beta Score\n",
    "\n",
    "## Commands\n",
    "\n",
    "* No specific technical commands were used in this lesson.\n",
    "\n",
    "## Summary\n",
    "\n",
    "* **Performance metrics** are essential for evaluating classification models.\n",
    "* A **Confusion Matrix** maps actual values against predicted values to categorize correct predictions and errors.\n",
    "* **Accuracy** can be misleading when working with **imbalanced datasets**.\n",
    "* **Precision** focuses on reducing **False Positives (FP)**.\n",
    "* **Recall** focuses on reducing **False Negatives (FN)**.\n",
    "* The **F-Beta Score** combines Precision and Recall using a harmonic mean and allows emphasis adjustment through the **beta parameter**.\n",
    "\n",
    "---\n",
    "\n",
    "## The Confusion Matrix\n",
    "\n",
    "A **Confusion Matrix** is the foundation for evaluating classification performance.\n",
    "\n",
    "For binary classification, it is a **2×2 matrix**:\n",
    "\n",
    "|                | Predicted 1 | Predicted 0 |\n",
    "|---------------|------------|------------|\n",
    "| **Actual 1**  | TP         | FN         |\n",
    "| **Actual 0**  | FP         | TN         |\n",
    "\n",
    "### Definitions\n",
    "\n",
    "* **True Positive (TP)**: Actual = 1, Predicted = 1  \n",
    "* **True Negative (TN)**: Actual = 0, Predicted = 0  \n",
    "* **False Positive (FP)**: Actual = 0, Predicted = 1  \n",
    "* **False Negative (FN)**: Actual = 1, Predicted = 0  \n",
    "\n",
    "Correct predictions: **TP + TN**  \n",
    "Incorrect predictions: **FP + FN**\n",
    "\n",
    "---\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "Accuracy measures overall correctness:\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "### Problem with Imbalanced Datasets\n",
    "\n",
    "Suppose:\n",
    "* 1000 records\n",
    "* 900 positive (1)\n",
    "* 100 negative (0)\n",
    "\n",
    "If a model predicts **1 for every record**, then:\n",
    "\n",
    "* TP = 900\n",
    "* TN = 0\n",
    "* Accuracy = 900 / 1000 = 90%\n",
    "\n",
    "The model achieves **90% accuracy** but completely fails to detect the negative class.\n",
    "\n",
    "Therefore, **Accuracy alone is unreliable** for imbalanced datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## Precision: Reducing False Positives\n",
    "\n",
    "Precision measures how many predicted positives are actually correct.\n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "### When to Use Precision?\n",
    "\n",
    "Use Precision when **False Positives are costly**.\n",
    "\n",
    "### Example: Spam Classification\n",
    "\n",
    "* Predicting spam incorrectly for an important email (FP) is a serious mistake.\n",
    "* Therefore, the priority is to **reduce False Positives**.\n",
    "* Precision should be optimized.\n",
    "\n",
    "---\n",
    "\n",
    "## Recall: Reducing False Negatives\n",
    "\n",
    "Recall measures how many actual positives are correctly identified.\n",
    "\n",
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "### When to Use Recall?\n",
    "\n",
    "Use Recall when **False Negatives are costly**.\n",
    "\n",
    "### Example: Disease Prediction\n",
    "\n",
    "* If a patient actually has a disease (1) but the model predicts healthy (0), that is a **False Negative**.\n",
    "* This is extremely dangerous.\n",
    "* Therefore, the priority is to **reduce False Negatives**.\n",
    "* Recall should be optimized.\n",
    "\n",
    "---\n",
    "\n",
    "## Stock Market Crash Example (Business Logic)\n",
    "\n",
    "If building a model to predict whether the stock market will crash:\n",
    "\n",
    "* **False Positive** → Predict crash, but no crash happens → Premature selling\n",
    "* **False Negative** → Predict no crash, but crash happens → Massive financial loss\n",
    "\n",
    "Since missing a crash is more catastrophic, the priority should be to **reduce False Negatives**, meaning **Recall** is more important.\n",
    "\n",
    "---\n",
    "\n",
    "## The F-Beta Score\n",
    "\n",
    "Sometimes both Precision and Recall matter.\n",
    "\n",
    "The **F-Beta Score** combines them using a harmonic mean:\n",
    "\n",
    "$$\n",
    "F_{\\beta}\n",
    "=\n",
    "(1 + \\beta^2)\n",
    "\\frac{Precision \\times Recall}\n",
    "{(\\beta^2 \\times Precision) + Recall}\n",
    "$$\n",
    "\n",
    "### Understanding Beta ($\\beta$)\n",
    "\n",
    "The $\\beta$ parameter controls emphasis:\n",
    "\n",
    "* **$\\beta = 1$ → F1 Score**  \n",
    "  Equal importance to Precision and Recall.\n",
    "\n",
    "* **$\\beta < 1$ (e.g., 0.5)**  \n",
    "  More weight on Precision.\n",
    "\n",
    "* **$\\beta > 1$ (e.g., 2)**  \n",
    "  More weight on Recall.\n",
    "\n",
    "---\n",
    "\n",
    "## Metric Comparison Summary\n",
    "\n",
    "| Metric     | Focus | Best Used When |\n",
    "|------------|-------|----------------|\n",
    "| Accuracy   | Overall correctness | Balanced datasets |\n",
    "| Precision  | Reduce FP | Spam detection, fraud alerts |\n",
    "| Recall     | Reduce FN | Medical diagnosis, crash prediction |\n",
    "| F1 Score   | Balance FP & FN | When both errors matter equally |\n",
    "| F-Beta     | Custom balance | When one error type matters more |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaway\n",
    "\n",
    "* Use **Accuracy** only for balanced datasets.\n",
    "* Use **Precision** when False Positives are costly.\n",
    "* Use **Recall** when False Negatives are costly.\n",
    "* Use **F-Beta** when you need a customizable balance between Precision and Recall."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
