{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d262ce3d",
   "metadata": {},
   "source": [
    "# Logistic Regression: Mathematical Intuition and Cost Function\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "* **Linear Regression** fails for classification because of **outliers** and unbounded outputs that can exceed 1 or drop below 0.\n",
    "* **Logistic Regression** squashes the output between 0 and 1 using the **Sigmoid Activation function**.\n",
    "* The standard linear regression cost function produces a **non-convex function** with multiple **local minima** when applied to logistic regression.\n",
    "* To ensure a **convex function** with a single **global minimum**, Logistic Regression uses a **Log Loss** cost function.\n",
    "* The **Convergence Algorithm (Gradient Descent)** iteratively minimizes the cost function by updating parameter values.\n",
    "\n",
    "---\n",
    "\n",
    "## Recap: Why Linear Regression Fails for Classification\n",
    "\n",
    "Linear Regression is unsuitable for classification for two key reasons:\n",
    "\n",
    "* **Outliers** can significantly shift the best fit line, altering predictions.\n",
    "* The linear hypothesis produces unbounded outputs that may be **greater than 1** or **less than 0**.\n",
    "\n",
    "To solve this, model outputs must be **bounded** within valid probability limits (0 to 1).\n",
    "\n",
    "---\n",
    "\n",
    "## The Sigmoid Activation Function\n",
    "\n",
    "The mathematical solution to squashing the output is the **Sigmoid Function**.\n",
    "\n",
    "### Sigmoid Formula\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Key properties:\n",
    "\n",
    "* The output is always strictly between **0 and 1**.\n",
    "* If $z \\ge 0$, then $\\sigma(z) \\ge 0.5$.\n",
    "* If $z < 0$, then $\\sigma(z) < 0.5$.\n",
    "\n",
    "---\n",
    "\n",
    "## The Logistic Regression Hypothesis\n",
    "\n",
    "In Linear Regression:\n",
    "\n",
    "$$\n",
    "f(x) = \\theta_0 + \\theta_1 x_1\n",
    "$$\n",
    "\n",
    "In Logistic Regression, we apply the sigmoid function:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "z = \\theta_0 + \\theta_1 x_1\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\sigma(\\theta_0 + \\theta_1 x_1)\n",
    "$$\n",
    "\n",
    "This transformation ensures predictions are bounded between 0 and 1.\n",
    "\n",
    "---\n",
    "\n",
    "## The Cost Function and Convexity\n",
    "\n",
    "In Linear Regression, the cost function is:\n",
    "\n",
    "$$\n",
    "J(\\theta_0, \\theta_1) =\n",
    "\\frac{1}{2m}\n",
    "\\sum_{i=1}^{m}\n",
    "\\left(h_\\theta(x^{(i)}) - y^{(i)}\\right)^2\n",
    "$$\n",
    "\n",
    "If this Mean Squared Error (MSE) is used with the sigmoid hypothesis, the resulting cost function becomes **non-convex**.\n",
    "\n",
    "### Why is this a problem?\n",
    "\n",
    "* Non-convex functions have multiple **local minima**.\n",
    "* Gradient Descent may get stuck in a local minimum.\n",
    "* The model may fail to reach the true **global minimum**.\n",
    "\n",
    "---\n",
    "\n",
    "## The Log Loss Cost Function\n",
    "\n",
    "To ensure convexity, Logistic Regression uses **Log Loss (Binary Cross-Entropy)**.\n",
    "\n",
    "### Conditional Form\n",
    "\n",
    "If $y = 1$:\n",
    "\n",
    "$$\n",
    "Cost = -\\log(h_\\theta(x))\n",
    "$$\n",
    "\n",
    "If $y = 0$:\n",
    "\n",
    "$$\n",
    "Cost = -\\log(1 - h_\\theta(x))\n",
    "$$\n",
    "\n",
    "### Combined Form\n",
    "\n",
    "$$\n",
    "Cost(h_\\theta(x), y)\n",
    "=\n",
    "- y \\log(h_\\theta(x))\n",
    "- (1 - y) \\log(1 - h_\\theta(x))\n",
    "$$\n",
    "\n",
    "### Final Logistic Regression Cost Function\n",
    "\n",
    "$$\n",
    "J(\\theta_0, \\theta_1)\n",
    "=\n",
    "-\\frac{1}{m}\n",
    "\\sum_{i=1}^{m}\n",
    "\\left[\n",
    "y^{(i)} \\log(h_\\theta(x^{(i)}))\n",
    "+\n",
    "(1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "This function is **strictly convex**, meaning it has:\n",
    "\n",
    "* One **global minimum**\n",
    "* No local minima\n",
    "* Reliable optimization using Gradient Descent\n",
    "\n",
    "---\n",
    "\n",
    "## Convergence Algorithm (Gradient Descent)\n",
    "\n",
    "The goal of training is to **minimize the cost function** by updating parameters $\\theta_0$ and $\\theta_1$.\n",
    "\n",
    "The update rule is:\n",
    "\n",
    "$$\n",
    "\\theta_j\n",
    ":=\n",
    "\\theta_j\n",
    "-\n",
    "\\alpha\n",
    "\\frac{\\partial}{\\partial \\theta_j}\n",
    "J(\\theta_0, \\theta_1)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\alpha$ = learning rate\n",
    "* $\\frac{\\partial}{\\partial \\theta_j}$ = partial derivative\n",
    "\n",
    "Because the Log Loss function is convex, Gradient Descent will correctly converge to the **global minimum**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
